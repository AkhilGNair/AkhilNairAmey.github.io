---
title: "Facebook Message Analysis"
output: html_document
---

Turns out Facebook messenger gives me a fairly solid dataset since 2009.  If you
download your own through settings on your profile, you should be able to run 
the same graphs off from your own data.

```{r setup, echo=FALSE}
# It is unclear why blank lines are stripped from code chunks

library(XML)
# wow, can u not
suppressPackageStartupMessages(library(purrr))
suppressPackageStartupMessages(library(data.table))
suppressPackageStartupMessages(library(magrittr))
library(tibble)
library(tidytext)
library(ggplot2)
# Helpers.r included as appendix
source("helpers.r")

# A colour palette from https://www.r-bloggers.com/the-paul-tol-21-color-salute/
palette = c(
  "#771155", "#AA4488", "#CC99BB", "#114477", "#4477AA", "#77AADD", "#117777", 
  "#44AAAA", "#77CCCC", "#117744", "#44AA77", "#88CCAA", "#777711", "#AAAA44", 
  "#DDDD77", "#774411", "#AA7744", "#DDAA77", "#771122", "#AA4455", "#DD7788"
)
palette = sample(palette, length(palette), replace = FALSE)

fmt_time = "%A, %B %d, %Y at %I:%M%p"

# The basic table
make_dt = function(i, u, t, m) {
  data.table(thread = i, user = u, timestamp = t, message = m)
}
```

First the data needs to be rvested (hah, good one Hadley) from the downloaded
`messages.htm` file.  This initially confused me but it turns out:

 - The messages are arrange into threads of max message count 10,000 per person
 - The messages are given out of order, with some batshit formatted timestamp
 - The message chain can't be reconstructed as seconds are missing from the 
 timestamp
    - Facebook why `:'(`
 - The conventions by which sometimes the user's alias is given, and sometimes 
 their ID is given is completely unclear to me.
    - It is also very irritating 

## Scraping

Xpaths make this pretty easy. What isn't easy is keeping everything less than 
`80` characters.

```{r scraping, echo=FALSE}
# Read in the whole messages htm file
html = XML::htmlTreeParse("messages.htm", useInternalNodes = TRUE)
# Scrape the threads
threads = XML::xpathSApply(html, '/html/body/div/div/div[@class="thread"]')

# Thread metadata
l_participants = lapply(threads, xpathSApply, 'text()', xmlValue)
# Splits comma seperated lists of people
l_participants = lapply(l_participants, split_people)
# Store these in a nested table for neatness
l_participants = purrr::map2(
  seq_along(l_participants), l_participants, 
  .f = ~tibble(thread = .x, people = list(.y))
)
dt_participants = purrr::reduce(l_participants, dplyr::bind_rows)
dt_participants = dt_participants %>%
  dplyr::rowwise() %>%
  dplyr::mutate(n_people = length(people)) %>%
  dplyr::mutate(convo = ifelse(n_people == 2, "private", "group")) %>%
  setDT()
```

The first thing we can scrap is a table that is boring to look at, but is useful
to identify participants of a conversation.  
Any thread with more than 2 participants is a group thread.
The rest of the analysis can be done grouped by thread to keep that separation

```{r participants}
# Not showing people as data.tables make list-columns hella ugly
DT::datatable(dt_participants[, .(thread, n_people, convo)])
```

It's then easy to use xpaths to grab the few fields offered to us
 - The sender of the message
 - The meta (which is just the timestamp [without seconds])
 - The actual message

```{r scrape_data, warning=FALSE, echo=FALSE, cache=TRUE}
# xpaths
# xpath_user = './div/div/span[@class="user"]'
# xpath_meta = './div/div/span[@class="meta"]'
# xpath_msg = './p'
# # Any field with useful info
# l_users = lapply(threads, xpathSApply, path = xpath_user, xmlValue)
# l_meta  = lapply(threads, xpathSApply, path = xpath_meta, xmlValue)
# l_msg   = lapply(threads, xpathSApply, path = xpath_msg,  xmlValue)
# # Bind into table retaining thread info
# # Cast the timestamp
# dt = pmap(.l = list(seq_along(l_users), l_users, l_meta, l_msg), make_dt)
# dt = rbindlist(dt)
# dt[, timestamp := as.POSIXct(strptime(timestamp, format = fmt_time))]
# # Somehow this first one is wrong. Not worth finding out why
# dt = dt[order(timestamp)][-1]
# dt[, year := lubridate::year(timestamp)]
# dt = dt_participants[, .(thread, convo)][dt, on = "thread"]
# save(dt, file = "cache/main_table.rda")
load(    file = "cache/main_table.rda")
```

And the nice table we're left with is... (ps, I'm removing my messages)

```{r fbtable, warning=FALSE}
DT::datatable(dt[, -"message"][, .(thread, year, timestamp, user, message = "hidden")])
```

So go nuts.  

### A free graph.

It's pretty easy to see where I stopped using MSN messenger.

```{r plot_msg_per_year, echo=FALSE, fig.height=4, fig.align='center'}
qplot(year, N, data = dt[, .N, year],
      geom = "col",
      main = "Messages per year", xlab = "Year", ylab = "Count") +
  theme_minimal()
```

## Users

This bit was extremely annoying as I had to actually do something by hand which
involved aliasing everyone who has changed their name.  I also matched up the 
facebook IDs to the standardised name by hand as I was already going in.  I did
this for the top 100 people I spoke to, and also the top 20 people who spoke to 
me per year 

 - Top 100 people: `dt[, .N, user][order(-N)][1:100]`
 - Top 20 per year: `dt[, .N, .(year, user)][order(-N), head(.SD, 20), year]`

People who were quite interesting to me here were people who I'd only frequently
spoken to in 1 or 2 years.  That's not to say my #dayones aren't interesting.

```{r aliasusers, echo=FALSE}
# Sadly you have to edit this csv by hand for best results of combining aliases
# I could ping Facebook but given I have to manually change nicknames I did the
# top 100 on the csv which took ~5 mins
dt_users = dt[, .N, user][order(-N)]
dt_users[, url := stringr::str_extract(user, "[0-9]+")]
dt_users[, url := paste0("www.facebook.com/", url)]
dt_users %>% write.csv("names.csv")
# Join on the aliases so I can group people messages from the same people
dt_users = fread("names_edit.csv")
dt = dt_users[, .(user, name)][dt, on = "user"]
```

### Years facet names

 - For the first two years apparently Facebook was only used for group chat
    - MSN hype was still strong
 - Robert, Lucy and Hanna are my most talked to buds
 - Bella's cropped up in the last year. Hi Bella!
 - Old housemates Connor and Tass dropped out. Camilla made a comes back when I
 met up with her this year. Hi Camilla!
 - Spoke to Steve a lot during 1st and 2nd year after which he dropped out. 
 Hi Steve!
 - 2012 and 2013 were some kind of rampage
 - My main group chat started in 2015

```{r plot_per_year, echo=FALSE, fig.width=14, fig.height=19, fig.align='center'}
# Look at most talked to people per year
dt_per_year = dt[name != "Akhil Nair", .N, .(convo, year, name)][order(year, -N), head(.SD, 20), .(year)]
dt_per_year = dt_per_year[year > 2008]
dt_per_year[, year := factor(year, levels = 2009:lubridate::year(Sys.Date()))]
dt_per_year = dt_per_year[, .(Infrequent = .N), name][Infrequent < 3][dt_per_year, on = "name"]
dt_per_year[, Infrequent := !is.na(Infrequent)]
dt_per_year[, Infrequent := ifelse(Infrequent == TRUE, "True", "False")]
dt_per_year[, convo := factor(convo, levels = c("group", "private"))]

# Already we can see a fun graph
ggplot(dt_per_year) +
  geom_col(
    aes(
      x = reorder(name, -N),
      y = N,
      fill = Infrequent,
      alpha = convo
    )
  ) +
  theme_minimal() +
  theme(
    text = element_text(size = 16),
    axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)
  ) +
  facet_wrap(~year, scales = 'free', ncol = 2) +
  labs(
    title = 'Most common people per year',
    x = 'Name',
    y = 'Count'
  ) +
  scale_fill_manual(values = c("steelblue", "#666666")) +
  scale_alpha_discrete(range = c(0.5, 1)) 
```

### Names facet years

 - Caroline's is pretty interesting - we move from private chat to group 
 messages pretty steadily
 - I talk to Nina online almost exclusively in a group chat
    - She didn't seem to care about us during university
    - No bar is an overstatement, but she wasn't in the top 20 people I spoke to 
   per year for those years, and the group chat makes up a good amount of that
 - Paul's group engagement is essentially some perfect exponential
 - Luke Facebook engagement jumped when he moved to the States
 - Isabel and I took a random 3 year break
    - We talked a surprising amount during second year!

```{r plot_per_person, echo=FALSE, fig.width=14, fig.height=11, fig.align='center'}
# A hilarious way I just found to make data.table chains more manageable...
idx_known4years = dt_per_year %>%
  `[`(, .(name, year)) %>% 
  unique() %>% 
  `[`(, .N, name) %>% 
  `[`(N > 4, name)

ggplot(dt_per_year[name %in% idx_known4years]) +
  geom_col(
    aes(
      x = year,
      y = N,
      alpha = convo
    ),
    fill = "steelblue"
  ) +
  theme_minimal() +
  theme(
    text = element_text(size = 16),
    axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)
  ) +
  facet_wrap(~name, scales = 'free', ncol = 3) +
  labs(
    title = 'Most common people per year',
    x = 'Name',
    y = 'Count'
  ) +
  scale_alpha_discrete(range = c(0.5, 1)) + 
  scale_x_discrete(drop = FALSE)
```

### Closing Notes

More to come! Plans I'm already working on include message/character ratios and
a quick ngram analysis (a keeno may have noticed `tidytext` sitting up there in
the library calls).

This is surprisingly time consuming, even only this much has taken about 6 hours
in total already!

The code will be available on github for you to run on your own data. It's 
surprisingly revealing...

Stay tuned...

```{r final, echo=FALSE, fig.align='center', fig.height=15, fig.width=14}
idx_user = dt[, .N, name][order(-N)][1:21, name]
dt[, convo := factor(convo, levels = c("group", "private"))]

ggplot(dt[name %in% idx_user][name != "Akhil Nair"]) +
  geom_histogram(aes(x = timestamp, fill = name, alpha = convo), 
                 binwidth = 60 * 60 * 24 * 14) +
  facet_grid(name ~ ., scale = "free_y", space = "free_y") +
  theme_minimal() +
  theme(
    text = element_text(size = 16),
    strip.text.y = element_text(angle = 0)
  ) +
  labs(
    x = "Time",
    y = "Message Count",
    Title = "Facebook Messages sent since I got Facebook Messenger",
    subtitle = "Top 20 people shown"
  ) +
  scale_fill_manual(values = palette) +
  scale_alpha_discrete(range = c(0.5, 1))
```
